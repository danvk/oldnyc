(These are some working notes from ~2013)

----

The NYC photos come from the NYPL's Milstein collection.

Start page:
http://digitalgallery.nypl.org/nypldigital/dgdivisionbrowseresult.cfm?div_id=hh

Matt K sent me a CSV file for their digitized photos with 43363 records.
(there are 63091 lines, but some records contain newlines)


Interesting fields:
 3	 59% (25721/43363)	CREATOR		Balgue, George L.--Photographer
 5	100% (43363/43363)	IMAGE_Permalink	http://digitalgallery.nypl.org/nypldigital/id?716312f
 6	100% (43363/43363)	DIGITAL_ID	716312f
11	100% (43363/43363)	RECORD_ID	406110
12	 62% (26943/43363)	Borough		Manhattan
13	 73% (31674/43363)	Address		??Amsterdam Avenue & La Salle Street
14	 84% (36327/43363)	Full Address	??Amsterdam Avenue & La Salle Street, Manhattan, NY
15	100% (43363/43363)	IMAGE_TITLE	??Manhattan: Amsterdam Avenue - La Salle Street
16	100% (43363/43363)	IMG_URL		http://images.nypl.org/index.php?id=716312f&t=w
17	 64% (27648/43363)	CREATED_DATE	1927

Other fields are rare or identical across all records.

Borough field breakdown:
  18084 Manhattan
  16420 ""
   5391 Brooklyn
   2253 Bronx
    845 ", Manhattan"
    257 Queens
    108 Staten Island
      3 Newtown Creek
      2 Governor's Island

Dates breakdown:
     1 1790's
     2 1820's
     5 1830's
    11 1850's
    69 1860's
    99 1870's
   149 1880's
   480 1890's
  1027 1900's
  1360 1910's
  6251 1920's
 10754 1930's
  2011 1940's
    30 1950's
    61 1960's
    28 1970's

My OldSF date range parser is able to understand 23375/25721=91% of the
date ranges.

About 75% of the images are from 1920-1949.

Date oddities:
This record is listed (in the CSV) as being created on "14793": http://digitalgallery.nypl.org/nypldigital/dgkeysearchdetail.cfm?strucID=288071&imageID=485982



Stuff to ask Maira/Matt:
1. What's the deal with odd dates like "14793" in the spreadsheet?
2. Most (~75%) of photos from 1920-1950, does that seem right?
3. Easy to get metadata on image height/width?
4. What's the largest version of the image available online?
5. Any way to get at just the image (and not the scan of its brown backing paper)?
6. There are some images (e.g. http://digitalgallery.nypl.org/nypldigital/id?700631f or http://digitalgallery.nypl.org/nypldigital/id?717711f) where there's a description and date on the image, but not on the site. Any way to get that? Have they not been digitized?
7. Are there any pre-rendered thumbnails?
    -> &t=r instead of &t=w
8. OK if I serve images directly off the NYPL site? (I used AWS for OldSF)



----

I uploaded the image records for OldSF to AppEngine so long ago that I have no
idea how I did it. They may have gone through the image labeling game, so that
there's some path dependence.

I should verify that geocoded addresses are in the borough they claim to be.
All of the dots around Greenwood Cemetery should be in Manhattan:
http://localhost:8080/#ll:40.652937|-74.002161&m:40.65581|-73.98758|15


----

I asked the NYPL folks whether there were higher-resolution images available.
David Riordan sent out links to "MrSID" files which are available on their
servers. This is an incredibly obscure image format which is designed for easy
scaling (ala Google Maps tiles). For better or for worse, it's the format that
the Milstein images were scanned in.

There's a sample MrSID file at:
http://lt.images.nypl.org/lizardtech/iserv/getdoc?cat=NYPL&item=ED/EDAA/9A2E/8602/11DD/8103/6EC3/9956/CD/EDAA9A2E-8602-11DD-8103-6EC39956CD08.sid

One option to convert it is to use "gdal_translate" with MrSID enabled.
Instructions for building this:
http://trac.osgeo.org/gdal/wiki/MrSID
http://trac.osgeo.org/gdal/wiki/BuildingOnMac

I set up an account with LizardTech and downloaded their SDK.

The SDK includes a "mrsiddecode" tool which works great!

$ mrsiddecode -i nyc.sid -o nyc.jpg
(takes ~10 seconds to convert)

The 3.5M MrSID file converts to either a 1.3MB JPG or a 25MB PNG file.

To download all the MrSID files, I can...
1. Fetch http://images.nypl.org/index.php?id=(image id)&t=d
2. Parse the '&item=' field out of the URL it redirects to
3. Fetch http://lt.images.nypl.org/lizardtech/iserv/getdoc?cat=NYPL&item=(ITEM).sid

It takes ~10 s to fetch each image and ~10 seconds to convert them, so w/ 40k
images, each of these would take 4-5 days.

----

Detecting the "cards" inside the Milstein images is going to be tricky.
It seems like a good candidate for edge detection + hough transform.

brew install PIL
sudo /usr/bin/easy_install scipy
sudo /usr/bin/easy_install -U scikit-image
  (needs Cython)

After some hacking, it seems like this will be a good strategy:
1. Shrink the images 5x in each dimension
2. Run canny edge detection
3. Zero out the margins (40px on each side)
4. Threshold at half the Otsu threshold.
5. Calculate sums across each column and each row.
6. Differentiate these.
7. Pick out the top 10 x- & y-coords.
8. Build rectangles out of each of these. Pick the brightest one (in canny image)

Another idea:
Canny finds x- and y-edges, then combines them into edges at any angle. But I
really do want x- and y-edges! Maybe I can use this intermediate data.

Another idea:
Binarize based on "brownness".
-> This works great!


Random sample of ten images:
images/733251f.jpg + perfect (2 cards)
images/700043f.jpg - (detects the entire image; background is too light)
images/715405f.jpg / (detects one correctly, other is the full top-half)
images/711018f.jpg + perfect (1 card)
images/715240f.jpg + perfect (2 cards)
images/702532f.jpg - (detects the entire image; background is too light)
images/725010f.jpg + perfect (1 card)
images/715153f.jpg + perfect (1 card)
images/710534f.jpg + perfect (1 card)
images/715591f.jpg + perfect (1 card)

-> 70% correct on the first go!

Made one change:
Instead of hard-coding brown as (178, 137, 90), I set it as the median color of
the image.

images/733251f.jpg +
images/700043f.jpg +
images/715405f.jpg +
images/711018f.jpg +
images/715240f.jpg +
images/702532f.jpg +
images/725010f.jpg +
images/715153f.jpg +
images/710534f.jpg +
images/715591f.jpg +

!!! 100% !!!



images/715426f.jpg doesn't work correctly
images/711019f.jpg (four small images)
images/715238f.jpg (combines two close images)

-----
2013-02-24

I've fetched essentially all the photos whose digital_ids end in 'f'
(34580/34726). This is ~80% of all the photos.


For a 'f' image, the sequence begins with
http://images.nypl.org/index.php?id=(image_id)&t=d

but this fails for non-'f' images.

Only a small fraction of the non-'f' images are on brown cards.
Using '&t=w' works fine.
http://images.nypl.org/index.php?id=3978353&t=w

Many of these non-'f' images don't have a "Zoom" button on the NYPL site, which
I take to mean that they have no associated MrSid file. But the same sequence
does work for some, so I'll try to get what I can.

I used Local Turk to generate test data for 100 images.

What's a good scoring function? There are two levels of "right":
1. Getting the correct number of photos
2. Getting the correct positions of the photos


-----
2013-03-07

With help from StackOverflow, I'm using a more principled way of detecting
regions (modeled after Matlab's RegionProps method) and am using
ndimage.binary_fill_holes to patch things up in the binarized image.

>= 10 : 16 are perfect
>= 20 : 15 are perfect

For some images (717271f.jpg), reducing the threshold from 20 -> 10 makes all
the difference. Backing off might work, but how to know if you only have 2/3?

For others (1507735.jpg), lowering the 95% Solidity threshold to 90% makes all
the difference.

-----
2013-04-10

Goal is to come up with a program that can reliably either:
1. Correctly detect photos in an image
2. Decline to do anything

i.e. have very few false positives.

After some tweaking, I came up with something that has these stats on the 120
training images:

     Safe: 5 (0.0417)
  Correct: 113 (0.9417)
    Wrong: 2 (0.0167)

i.e. it partitions 94% correctly, takes a conservative pass on 4% and gets 1.7%
incorrect.

In this last category, one is because of a bad binarization (one
image becomes two). The other is because only 2/3 photos in an image are
detected (the third is sepia and, unlike the other two, doesn't have a white
border). These are both lame, but the former is lamer.

2013-04-13
Goals:
- Last tweaks to photo-finding algorithm.
- Run on 100-200 new random photos from the full collection.
- Take a crack at removing white borders.
- Kick off a run on the full set.

#26: pass, but shouldn't be?
#36: bad pass
#39: huge border!
#45: bad pass
#46: bad pass
#49: bad pass
#61: bad pass
#62: bad pass
#70: bad pass
#77: bad pass?

Setting a pass threshold in terms of solidity (instead of height/width
fraction) might make more sense.

Going over a second sample of 100 images.
-- lots more passes on this set --
8/100 images2/1596925.jpg -- false positive
14/100 (pass) images2/701333f.jpg -- bad pass

-> Looked really good!
-> The problems were almost all with the early images; maybe a different set?
  -> Fortunately, the really bad ones don't have addresses!

Cropping an image takes ~1s on my laptop.

Started on the full 40k run at 3:14PM.
... scratch that! My workstation has 12 cores, so why not run a few copies in parallel?
Restarted w/ four tasks at 3:22PM.

There were a few issues with "images of death" which took down a process. I
eventually wrote a small HTTP server to distribute tasks and finished the run
of 36475 images in the evening.

417664, 465617, 465553, 465601, 417658: schematics, maybe I can spot them via "illustration"?

1113236 wide format, pass is best

After looking through 200 random images, I didn't see any egregious errors.
-> We're good to go!

With four jobs, I'm cropping 6.5images/sec.

Total disk usage:
  58G originals
  18G crops
  (23G crops + accepted originals)
  75.28G total

Commands to do everything:

(Serve up paths to images which haven't had find_pictures.py run on them)
$ ls milstein/*.jpg | ./list-incomplete-tasks.py detected-photos.txt | ./task-server.py

(Run a few find_pictures.py tasks)
$ ./find_pictures.py http://localhost:8765 | tee 1detected-photos.txt
$ ./find_pictures.py http://localhost:8765 | tee 2detected-photos.txt
...

(Combine detected photos)
$ cat ?detected* | sort | uniq > detected-photos.txt

(Serve up cropping tasks)
$ ./task-server.py detected-photos.txt

(Run a few extract-photos.py instances)
$ ./extract-photos.py http://localhost:8765 ~/milstein-crops | tee -a ~/milstein-crops/1crops.txt
$ ./extract-photos.py http://localhost:8765 ~/milstein-crops | tee -a ~/milstein-crops/2crops.txt

(Combine crops files)
$ cat milstein-crops/?crops.txt | sort | uniq > milstein-crops/crops.txt

(Convert to JSON)
$ ./crops-to-json.py crops.txt > photos.json


There are three images that the find_pictures algorithm failed on:
705891f.jpg
706199f.jpg
712817f.jpg

These all say "Plate Wanting" in big black type on a white background.

Here's what I've got:
- 37632 full-size JPG files (58GB) from the Milstein collection (converted MrSID files).
- 37629 images on which I ran the photo detector.
- 34194 images where a photo was detected & extracted.
- 52369 smaller JPGs (19GB) for photos that I extracted from the originals.

The algorithm declined to do anything with ~9% of the original images.

Here's the distribution of # of detected photos per original image:
0: 3435
1: 22246
2: 6545
3: 4662
4: 689
5: 34
6: 11
7: 2
8: 4
9: 1

The image on which nine photos were detected really does have nine photos!


2013-04-23
With the photos extracted, it's time to move back to geocoding.

I found a bunch of photos of the old Google NY building. But it took the
descriptions for me to confirm what I was looking at (particularly when it was
a hole in the ground). Having these in OldNYC would make it a way more
compelling product.

- What fraction of photos have descriptions?
  -> 86% have a metadata image; 66% have one with extra information (not just date/place).
- Is the typewriter font consistent?
  -> Seems to be!

(730166f -- great view of Central Park http://digitalgallery.nypl.org/nypldigital/id?730166f)

------
2013-05-08/12

I want to look into some of the geocoding issues.
Looks like I'm only geocoding 'A & B' and forgot I was doing that!

Next step: Look at the non-city/borough part of the address for a sample of 100
to get a sense of what these look like.

 38 "A (&|and) B, Borough, NY"   69th Street & York Avenue, Manhattan, NY
  6 (missing)
  3 "Place Name, Borough, NY"    Kill Van Kull, Staten Island, N.Y.
  1 "Address .*, Borough, NY"    929 Park Avenue. Near Eighty-first Street., Manhattan, NY
  1 "A - B, .*, Borough, NY"     38th Street (West) - Twelfth Avenue, northeast corner -, Manhattan, NY
  1 "A, Borough, NY"             Elizabeth Street, Manhattan, NY
  1 (too vague)                  General view of fishing facilities., Manhattan, NY

Notes:
- Try removing the "(West)" to improve geocoding ('62nd Street (West) & Amsterdam Avenue')
- "N.Y." and "NY" are freely intermixed.
- Sometimes there are lots of extra spaces before a comma
- Sometimes have county + borough, e.g. "Church of St. Andrew, Richmond, Staten Island, N.Y."

In any case, the vast majority (84%) of the addresses are cross-streets with
either a '&' or 'and' in them. It make sense to start by getting the geocoding
really good for just this class of records.

The '&|and' check gets me:
  Parsed 30337 / 36328 = 0.8351 records


There are still some addresses floating in free text:
  The  Mannados, 17 East  Ninety-seventh Street, northwest corner Madison Avenue, Manhattan, NY
  The  Maranamay, 611 West 112th Street , Manhattan, NY
  The Apartment House Surrounded By Gardens, St. George Gardens, Inc. 90 St. Marks(sic) Place, St. George, Staten Island, N.Y.

Parsed 34023 / 36328 = 0.9366 records

There are 2305 remaining.

------
2013-05-27

I want to implement "is this geocode in the right borough" logic and look at
some of the failures.

I found 911 examples of borough mismatches. The vast majority are geocoded into
Brooklyn when they should have been in Manhattan:

 592 Found Brooklyn expected Manhattan
  84 Found Queens expected Manhattan
  76 Found Manhattan expected Brooklyn
  39 Found None expected Manhattan
  36 Found Bronx expected Manhattan
  34 Found Manhattan expected Queens
  18 Found Queens expected Brooklyn
   9 Found Brooklyn expected Queens
   6 Found Bronx expected Brooklyn
   5 Found Brooklyn expected Staten Island
   3 Found None expected Brooklyn
   3 Found Manhattan expected Bronx
   2 Found Staten Island expected Manhattan
   2 Found Queens expected Bronx
   2 Found Manhattan expected Staten Island
   1 Found Queens expected Staten Island
   1 Found None expected Queens
   1 Found Bronx expected Queens

These are typically cross-streets which could be in either borough. The
locatable string is always listed as something like '20th Street (East) & 4th
Avenue, Manhattan, NY', however, so I'm not sure why they show up in the wrong
place.


------
2013-11-29

I want to increase the coverage of geocoding for Brooklyn images.

There are 5391 images with "Brooklyn" as their borough.

Generate all geocodes (takes ~1 minute):
$ ./generate-geocodes.py --coders milstein --pickle_path nyc/records.pickle --output_format records.js --geocode > /tmp/records.json
$ cd nyc
$ ./coverage-by-borough.py /tmp/records.json

0.3092 (  746 /  2413) Bronx (5 to wrong borough)
0.3167 ( 1880 /  5936) Brooklyn (103 to wrong borough)
0.5728 (12494 / 21812) Manhattan (753 to wrong borough)
0.4009 ( 1439 /  3589) Queens (45 to wrong borough)
0.1784 (  439 /  2461) Staten Island (8 to wrong borough)
0.0000 (    0 /  7152) Unknown (0 to wrong borough)

So Brooklyn has about half the geocode success rate that Manhattan does.

I made the coverage-by-borough script dump Brooklyn failures into a temp
file. Digging into one of these:
703739f Columbia Street & State Street, Brooklyn, NY

$ ./generate-geocodes.py --ids_filter 703739f --coders milstein --pickle_path nyc/records.pickle --output_format records.js --geocode
Failure: Columbia Street and State Street -> (40.689251, -73.988736)

That lat/lon is a few blocks down the street, at a lower degree of accuracy
than indicates a successful cross-street geocode. This is strange because
when I type "Columbia Street & State Street, Brooklyn, NY" into Google
Maps, it gets it. I modified geocoder.py to print the cache files it checks
and deleted the one for this record to force it back to Google Maps.

At this point, I discover that the Google Maps geocoding API I've been
using for the past ~5 years has ceased to exist! I need to upgrade to v3 of
the geocoding API.

This is a good opportunity to rewrite the geocoder. This grew organically
while I developed OldSF and I shoehorned NYC geocoding into it.

Things I'd like to change:
- Each coder should be responsible for determining whether its own
  locatable string was geocoded successfully.
- Get rid of locatable.py. Coders should simply output a string.
- Coders should output a complete locatable string, including city name.
- generate-geocodes should have a way to build on existing successful
  geocodes. Once there's a successful geocode, it should never be lost.
- There should be a consistent "debug mode" or info logging.
- The "coder cache" hasn't been that useful.
- The different output types of generate-geocodes. It should do one thing.

Things I'd like to keep:
- The "geocache" layer.
- Separate coders.
- A toggle for whether to hit the network or not.

Rerunning geocodes using the new API is helping the Brooklyn geocodes, even
without algorithm changes. With 5000 new geocodes (my per-IP limit is
2500/day), I've picked up 353 new images in Brooklyn, raising the recall from
31.67% --> 37.62%.


2013-12-01
After re-running all the geocodes, per-borough coverage has improved across
the board:
0.6581 ( 1588 /  2413) Bronx (26 to wrong borough)
0.7011 ( 4162 /  5936) Brooklyn (154 to wrong borough)
0.7362 (16057 / 21812) Manhattan (981 to wrong borough)
0.6890 ( 2473 /  3589) Queens (82 to wrong borough)
0.4738 ( 1166 /  2461) Staten Island (11 to wrong borough)
0.0000 (    0 /  7152) Unknown (0 to wrong borough)

Geocodes of Brooklyn images have gone from 32% --> 70%!
I could get another 13-14% by geocoding the top 100 addresses by hand.


2013-12-07
Goal today is to see the new geocodes on a map and get some hard numbers about
JSON file size before & after clumping by neighborhoods.

To regenerate the latlons.js file, I had to use the "photos.pickle" file, which
uses extracted photos rather than the raw Milstein records:
./generate-geocodes.py --coders milstein --pickle_path nyc/photos.pickle --output_format lat-lons.js --geocode > viewer/nyc-lat-lons.js

Serve the 600px images & thumbnails from a Python server:
cd /Volumes/teradan/Milstein\ Images/
python -m SimpleHTTPServer 8001

Start the local dev server:
cd viewer
dev_appserver.py .

And visit http://localhost:8080

To upload image records:
./upload_to_appengine.py --pickle_path nyc/photos.pickle --lat_lons_js_path viewer/nyc-lat-lons.js --image_sizes_path nyc-image-sizes.txt
(takes ~10 minutes)

Be sure to flush memcache on the development instance after you do this! To do
so, visit http://localhost:8000/memcache

Sample command to view a record using the interactive console on the admin site:

  import os
  import pprint
  import app

  from google.appengine.api import memcache
  from google.appengine.ext import db

  #config = db.create_config(read_policy=db.EVENTUAL_CONSISTENCY)
  #r = app.ImageRecord.get_by_key_name('724963f-a', config=config)

  rs = app.GetImageRecords(['724963f-a'])

  r = r['724963f-a']
  print r.title
  print r.width
  print r.height

There are ~2000 geocoded images which have a neighborhood of 'None'. What up
with that?


What are the limits to geocoding?
There are 43363 records. Of these, 7054 are missing location data.
--> 36309 max
--> I have 25657 / 36309 = 70%.

For some, the geocodable data is in a different field, e.g. 723915f. This has “Wall Street” as its address, which is not locatable, but the “alternate title” field is “40 Wall Street, Bank of Manhattan Co.", which obviously is.


2/23/2014
References for doing an image grid layout/UI like Google Image Search:
http://stackoverflow.com/questions/16767195/ui-similar-to-google-image-search
http://blog.vjeux.com/2012/image/image-layout-algorithm-google-plus.html
http://tympanus.net/codrops/2013/03/19/thumbnail-grid-with-expanding-preview/
http://tympanus.net/Tutorials/ThumbnailGridExpandingPreview/

Old URL: http://digitalgallery.nypl.org/nypldigital/id?712208f
New URL: http://digitalcollections.nypl.org/items/image_id/712208f
